---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.16.4
  name: schedulingconfigtemplates.tensor-fusion.ai
spec:
  group: tensor-fusion.ai
  names:
    kind: SchedulingConfigTemplate
    listKind: SchedulingConfigTemplateList
    plural: schedulingconfigtemplates
    singular: schedulingconfigtemplate
  scope: Cluster
  versions:
  - additionalPrinterColumns:
    - jsonPath: .spec.placement.mode
      name: Mode
      type: string
    - jsonPath: .spec.placement.allowLocalGPU
      name: Allow Local GPU
      type: string
    - jsonPath: .spec.hypervisor.autoFreezeAndResume.autoFreeze.enable
      name: AutoFreeze
      type: string
    name: v1
    schema:
      openAPIV3Schema:
        description: SchedulingConfigTemplate is the Schema for the schedulingconfigtemplates
          API.
        properties:
          apiVersion:
            description: |-
              APIVersion defines the versioned schema of this representation of an object.
              Servers should convert recognized schemas to the latest internal value, and
              may reject unrecognized values.
              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources
            type: string
          kind:
            description: |-
              Kind is a string value representing the REST resource this object represents.
              Servers may infer this from the endpoint the client submits requests to.
              Cannot be updated.
              In CamelCase.
              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
            type: string
          metadata:
            type: object
          spec:
            description: Place the workload to right nodes and scale smart.
            properties:
              autoScaling:
                description: scale the workload based on the usage and traffic
                properties:
                  autoSetLimits:
                    description: |-
                      layer 1 vertical auto-scaling, turbo burst to existing GPU cards quickly
                      VPA-like, aggregate metrics data <1m
                    properties:
                      enable:
                        type: boolean
                      evaluationPeriod:
                        type: string
                      extraTFlopsBufferRatio:
                        type: string
                      ignoredDeltaRange:
                        type: string
                      maxRatioToRequests:
                        description: the multiplier of requests, to avoid limit set
                          too high, like 5.0
                        type: string
                      prediction:
                        properties:
                          enable:
                            type: boolean
                          historyDataPeriod:
                            type: string
                          model:
                            type: string
                          predictionPeriod:
                            type: string
                        type: object
                      scaleUpStep:
                        type: string
                      targetResource:
                        description: target resource to scale limits, such as "tflops",
                          "vram", or "all" by default
                        type: string
                    type: object
                  autoSetReplicas:
                    description: |-
                      layer 2 horizontal auto-scaling, scale up to more GPU cards if max limits threshold hit
                      HPA-like, aggregate metrics data 1m-1h (when tf-worker scaled-up, should also trigger client pod's owner[Deployment etc.]'s replica increasing, check if KNative works)
                    properties:
                      enable:
                        type: boolean
                      evaluationPeriod:
                        type: string
                      scaleDownCoolDownTime:
                        type: string
                      scaleDownStep:
                        type: string
                      scaleUpCoolDownTime:
                        type: string
                      scaleUpStep:
                        type: string
                      targetTFlopsOfLimits:
                        type: string
                    type: object
                  autoSetRequests:
                    description: |-
                      layer 3 adjusting, to match the actual usage in the long run, only for N:M remote vGPU mode, not impl yet
                      Adjust baseline requests to match the actual usage in longer period, such as 1day - 2weeks
                    properties:
                      aggregationPeriod:
                        type: string
                      enable:
                        type: boolean
                      evaluationPeriod:
                        type: string
                      extraBufferRatio:
                        description: the request buffer ratio, for example actual
                          usage is 1.0, 10% buffer will be 1.1 as final preferred
                          requests
                        type: string
                      percentileForAutoRequests:
                        type: string
                      prediction:
                        properties:
                          enable:
                            type: boolean
                          historyDataPeriod:
                            type: string
                          model:
                            type: string
                          predictionPeriod:
                            type: string
                        type: object
                      targetResource:
                        description: target resource to scale requests, such as "tflops",
                          "vram", or "all" by default
                        type: string
                    type: object
                type: object
              hypervisor:
                description: single GPU device multi-process queuing and fair scheduling
                  with QoS constraint
                properties:
                  autoFreezeAndResume:
                    description: |-
                      additional layer to save VRAM, auto-freeze memory and cool down to RAM and Disk
                      Hypervisor will monitor and trigger freeze of inactive workers, Operator should mark them as scaled-to-zero and release the GPU pool resources, don't scale down CPU client part, so that they can continue to serve the traffic or scale down by other auto-scaling solutions like KEDA/KNative
                    properties:
                      autoFreeze:
                        items:
                          properties:
                            enable:
                              type: boolean
                            freezeToDiskTTL:
                              type: string
                            freezeToMemTTL:
                              type: string
                            qos:
                              enum:
                              - low
                              - medium
                              - high
                              - critical
                              type: string
                          type: object
                        type: array
                      intelligenceWarmup:
                        properties:
                          enable:
                            type: boolean
                          historyDataPeriod:
                            type: string
                          model:
                            type: string
                          predictionPeriod:
                            type: string
                        type: object
                    type: object
                  multiProcessQueuing:
                    description: |-
                      Hypervisor will move low priority jobs to pending queue if GPU is full
                      This config can adjust hypervisor's queueing behavior to balance the co-scheduling CUDA calls
                    properties:
                      enable:
                        type: boolean
                      interval:
                        type: string
                      queueLevelTimeSlices:
                        items:
                          type: string
                        type: array
                    type: object
                type: object
              placement:
                description: place the client or worker to best matched nodes
                properties:
                  allowUsingLocalGPU:
                    default: true
                    type: boolean
                  celFilters:
                    description: CEL-based GPU filters for advanced filtering logic
                    items:
                      description: |-
                        CELFilterConfig defines the configuration for CEL-based filtering

                        example:
                        ```yaml
                          - name: "avoid-overloaded-gpus"
                            expression: "gpu.available.tflops > 0.5 && size(gpu.runningApps) < 3"
                            priority: 100
                          - name: "prefer-specific-model"
                            expression: "gpu.gpuModel.startsWith('NVIDIA') && gpu.labels.has('gpu-tier') && gpu.labels['gpu-tier'] == 'premium'"
                            priority: 50

                        ```
                      properties:
                        expression:
                          description: |-
                            CEL expression for filtering GPUs
                            The expression should return a boolean value
                            Available variables: gpu, workerPodKey, request
                          type: string
                        name:
                          description: Name for this filter (for debugging/logging)
                          type: string
                        priority:
                          default: 0
                          description: Priority for this filter (higher priority filters
                            run first)
                          type: integer
                      required:
                      - expression
                      type: object
                    type: array
                  gpuFilters:
                    items:
                      description: "GPUFilter is to select eligible GPUs for scheduling.\n\nexample:\n```yaml\n-
                        type: avoidTooMuchConnectionsOnSameGPU\nparams:\n\n\tconnectionNum:
                        150\n\n- type: avoidDifferentZone\nparams:\n\n\t# by default,
                        GPU worker will be scheduled into the same zone as CPU Client
                        Pod to align AZ and improve performance\n\ttopologyKey: topology.kubernetes.io/zone\n\n```"
                      properties:
                        params:
                          type: object
                          x-kubernetes-preserve-unknown-fields: true
                        type:
                          type: string
                      type: object
                    type: array
                  mode:
                    default: CompactFirst
                    enum:
                    - CompactFirst
                    - LowLoadFirst
                    type: string
                required:
                - mode
                type: object
              reBalancer:
                description: |-
                  avoid hot GPU devices and continuously balance the workload
                  implemented by trigger a simulation scheduling and advise better GPU nodes for scheduler
                properties:
                  enable:
                    type: boolean
                  interval:
                    type: string
                  reBalanceCoolDownTime:
                    type: string
                  threshold:
                    properties:
                      matchAny:
                        type: object
                        x-kubernetes-preserve-unknown-fields: true
                    type: object
                type: object
            required:
            - placement
            type: object
          status:
            description: SchedulingConfigTemplateStatus defines the observed state
              of SchedulingConfigTemplate.
            type: object
        type: object
    served: true
    storage: true
    subresources:
      status: {}
